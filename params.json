{
  "name": "topic model of book review ",
  "tagline": "",
  "body": "# A case of LADvis\r\n\r\nThe result of LAD visualization: [http://jiangshiying.github.io/vis](http://jiangshiying.github.io/vis)\r\n\r\n**Data collection**\r\n\r\nBooks review data from Amazon which includes 3189 positive and negative reviews.\r\nData source: [https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html](https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html)\r\n\r\n    book<-read.csv(\"assignment8/review.csv\",header=T,stringsAsFactors = F)\r\n\r\n    dim(book)\r\n\r\n    review<-book$review\r\n\r\n**Data preprocessing**\r\n\r\nFirst of all, load all package that used in this case\r\n    library(tm)\r\n    library(ggdendro)\r\n    library(fpc) \r\n    library(lda)\r\n    library(dplyr)\r\n    require(magrittr)\r\n    library(stringr)\r\n    library(LDAvis)\r\n    library(servr) \r\nCreate a stopword list. In this case, besides words in stopwords dictionary I add \"book\" in stopwords list.Â  \r\n\r\n    #create stop word set, including general stop word and \"book\"\r\n\r\n    stop_words=stopwords(\"SMART\")\r\n    stop_words=c(stop_words,\"book\")\r\n    stop_words=tolower(stop_words)\r\n\r\nThen , tokenize the text, remove the stopwords and remove the words that occur less than 5 times.\r\n\r\n    review <- gsub(\"'\", \"\", review) # remove apostrophes\r\n    review <- gsub(\"[[:punct:]]\", \" \", review)  # replace punctuation with space\r\n    review <- gsub(\"[[:cntrl:]]\", \" \", review)  # replace control characters with space\r\n    review <- gsub(\"^[[:space:]]+\", \"\", review) # remove whitespace at beginning of documents\r\n    review <- gsub(\"[[:space:]]+$\", \"\", review) # remove whitespace at end of documents\r\n    review <- gsub(\"[^a-zA-Z -]\", \" \", review) # allows only letters\r\n    review <- tolower(review)  # force to lowercase\r\n\r\n    review<-review[review!=\"\"]\r\n\r\n    doc.list<-strsplit(review,\"[[:space:]]+\")\r\n\r\n    #create dictionary\r\n    term.table <- table(unlist(doc.list))\r\n    term.table <- sort(term.table, decreasing = TRUE)\r\n\r\n    # remove terms that are stop words or occur fewer than 5 times:\r\n    del <- names(term.table) %in% stop_words | term.table < 5\r\n    term.table <- term.table[!del]\r\n    term.table <- term.table[names(term.table) != \"\"]\r\n    vocab <- names(term.table)\r\n\r\nUse function to formalize corpus for LAD analysis.\r\n\r\n    get.terms <- function(x) {\r\n      index <- match(x, vocab)\r\n      index <- index[!is.na(index)]\r\n      rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\r\n    }\r\n    documents <- lapply(doc.list, get.terms)\r\n\r\n**Using the R package 'lda' for model fitting**\r\nUse LAD method to anaysis for 20 topics and iterate 5000 times, and record the time that spent.\r\n    K <- 20 #topic number\r\n    G <- 5000 #iteration number \r\n    alpha <- 0.02\r\n    eta <- 0.02\r\n\r\n    set.seed(357)\r\n    t1 <- Sys.time()\r\n    fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \r\n                                   num.iterations = G, alpha = alpha, \r\n                                   eta = eta, initial = NULL, burnin = 0,\r\n                                   compute.log.likelihood = TRUE)\r\n    t2 <- Sys.time()\r\n    t2-t1\r\n\r\n**visualization**\r\nUse LADvis package to do the visualization. \r\n\r\n    #document-term distribution matrix\r\n    theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\r\n    #term-word distribution matrix\r\n    phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\r\n    doc.length <- sapply(documents, function(x) sum(x[2, ])) \r\n\r\n    # create the JSON object to feed the visualization:\r\n    json <- createJSON(phi = phi,\r\n                     theta = theta,\r\n                     doc.length = doc.length,\r\n                     vocab = vocab,\r\n                     term.frequency = term.frequency)\r\n\r\n    serVis(json, out.dir = 'vis', open.browser = TRUE)\r\n\r\nCode:\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}